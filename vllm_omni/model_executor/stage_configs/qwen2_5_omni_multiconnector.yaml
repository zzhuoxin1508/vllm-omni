# stage config for running qwen2.5-omni with architecture of OmniLLM.

# The following config has been verified on 1x H100-80G GPU.
stage_args:
  - stage_id: 0
    stage_type: llm  # Use llm stage type to launch OmniLLM
    runtime:
      process: true            # Run this stage in a separate process
      devices: "0"            # Visible devices for this stage (CUDA_VISIBLE_DEVICES/torch.cuda.set_device)
      max_batch_size: 1
    engine_args:
      model_stage: thinker
      model_arch: Qwen2_5OmniForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.8
      enforce_eager: true  # Now we only support eager mode
      trust_remote_code: true
      engine_output_type: latent
      enable_prefix_caching: false
    is_comprehension: true
    final_output: true
    final_output_type: text
    default_sampling_params:
      temperature: 0.0
      top_p: 1.0
      top_k: -1
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.1
    # Distributed connector configuration (optional)
    output_connectors:
      to_stage_1: mooncake_connector
  - stage_id: 1
    stage_type: llm  # Use llm stage type to launch OmniLLM
    runtime:
      process: true
      devices: "1"
      max_batch_size: 1
    engine_args:
      model_stage: talker
      model_arch: Qwen2_5OmniForConditionalGeneration
      worker_type: ar
      scheduler_cls: vllm_omni.core.sched.omni_ar_scheduler.OmniARScheduler
      gpu_memory_utilization: 0.8
      enforce_eager: true
      trust_remote_code: true
      enable_prefix_caching: false
      engine_output_type: latent
    engine_input_source: [0]
    custom_process_input_func: vllm_omni.model_executor.stage_input_processors.qwen2_5_omni.thinker2talker
    default_sampling_params:
      temperature: 0.9
      top_p: 0.8
      top_k: 40
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.05
      stop_token_ids: [8294]
    # Distributed connector configuration (optional)
    input_connectors:
      from_stage_0: mooncake_connector
    output_connectors:
      to_stage_2: mooncake_connector
  - stage_id: 2
    stage_type: llm  # Use llm stage type to launch OmniLLM
    runtime:
      process: true
      devices: "2"            # Example: use a different GPU than the previous stage; use "0" if single GPU
      max_batch_size: 1
    engine_args:
      model_stage: code2wav
      model_arch: Qwen2_5OmniForConditionalGeneration
      worker_type: generation
      scheduler_cls: vllm_omni.core.sched.omni_generation_scheduler.OmniGenerationScheduler
      gpu_memory_utilization: 0.3
      enforce_eager: true
      trust_remote_code: true
      enable_prefix_caching: false
      max_num_batched_tokens: 32768
      engine_output_type: audio
    engine_input_source: [1]
    final_output: true
    final_output_type: audio
    default_sampling_params:
      temperature: 0.0
      top_p: 1.0
      top_k: -1
      max_tokens: 2048
      seed: 42
      detokenize: True
      repetition_penalty: 1.1
    # Distributed connector configuration (optional)
    input_connectors:
      from_stage_1: mooncake_connector

# Top-level runtime config (concise): default windows and stage edges
runtime:
  enabled: true
  defaults:
    window_size: -1             # Simplified: trigger downstream only after full upstream completion
    max_inflight: 1             # Simplified: process serially within each stage

  # Distributed connectors configuration (optional)
  # More connectors will be supported in the future.
  connectors:
    # Mooncake connector for cross-node/intra-node communication
    mooncake_connector:
      name: MooncakeConnector
      extra:
        host: "127.0.0.1"
        metadata_server: "http://10.90.67.86:8080/metadata"
        master: "10.90.67.86:50051"
        segment: 512000000    # 512MB
        localbuf: 64000000     # 64MB
        proto: "tcp"

    # Yuanrong connector for cross-node/intra-node communication
    yuanrong_connector:
      name: YuanrongConnector
      extra:
        host: "127.0.0.1"
        port: "35000"

    # SharedMemory connector for intra-node communication
    # Alternative SHM connector with different threshold
    shared_memory_connector:
      name: SharedMemoryConnector
      extra:
        shm_threshold_bytes: 65536 # 64KB threshold

  edges:
    - from: 0                   # thinker → talker: trigger only after receiving full input (-1)
      to: 1
      window_size: -1
    - from: 1                   # talker → code2wav: trigger only after receiving full input (-1)
      to: 2
      window_size: -1
